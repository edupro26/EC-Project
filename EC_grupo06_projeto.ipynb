{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Engenharia do Conhecimento 2023/2024\n",
    "\n",
    "## Project: Classification and Regression models with *Thyroid disease Data Set*\n",
    "\n",
    "#### Group 6:\n",
    "\n",
    "- Eduardo Proen√ßa 57551\n",
    "- Tiago Oliveira 54979\n",
    "- Bernardo Lopes 54386"
   ],
   "id": "37af115655e6ef16"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Summary\n",
    "\n",
    "1. Data Processing\n",
    "\n",
    "    1. Creating a Data Frame\n",
    "    2. Data investigation\n",
    "    3. Encoding Data\n",
    "    4. Splitting into training and testing set\n",
    "    5. Imputing missing values\n",
    "    6. Scaling Data\n",
    "    \n",
    "2. Classification\n",
    "\n",
    "    1. Feature Selection\n",
    "    2. KFold Cross validation\n",
    "    3. Classification models\n",
    "       \n",
    "3. Model Selection\n",
    "\n",
    "    1. Hyperparameter tuning\n",
    "    2. Model Testing\n",
    "\n",
    "4. Conclusion"
   ],
   "id": "8735b013813cf85e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1. Data Processing\n",
    "\n",
    "When constructing a machine learning model, data processing is an important step. We need to ensure that our data set is properly processed so that it can be used in the best possible way by diferent classification models."
   ],
   "id": "7a3fe0f7bf074860"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.1 Creating a Data Frame",
   "id": "79a91cdf41098297"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The first step is load our data set. For that, we can use the [Pandas](https://pandas.pydata.org) Python Library, to read the file \"proj-data.csv\", which contains the data set we will be using in this project and build a DataFrame",
   "id": "a5f79513063bfc4b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data set\n",
    "df_thyroid = pd.read_csv('proj-data.csv')\n",
    "df_thyroid.shape"
   ],
   "id": "f2edb1e685a0e2d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_thyroid.head()",
   "id": "3b1ab9a330192af9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2 Data investigation",
   "id": "d58cfed3fb9fc2a3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After building our DataFrame, it's important to do some investigation, so we can gain a better understanding of our data.\n",
    "\n",
    "The first detail we notice is that there are missing values represented by '?'. These will be handled later in this notebook, for now\n",
    "we will just replace them with NaN, so they can be identified."
   ],
   "id": "f8a4977afe7048f3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace missing values with NaN\n",
    "df = df_thyroid.copy()\n",
    "df.replace('?', np.nan, inplace=True)\n",
    "\n",
    "df.info()"
   ],
   "id": "d25207feac655236",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Looking at the number of different values of each column.",
   "id": "1be6aba230088054"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Uniques values:\")\n",
    "for col in df_thyroid.columns:\n",
    "    unique_vals = df_thyroid[col].nunique()\n",
    "    print(f'{col} = ', unique_vals)"
   ],
   "id": "b917f0b00d08f6bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's look at the number of missing values of each column.",
   "id": "8beed41a0ebdb11"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "5e9ad342bd5f9014"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df.isna().sum()",
   "id": "6f21aa7626f43110",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "db1df5f7e68a23bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "After some investegation we know that our data set has two types of columns: binary columns which have only two possible non-numeric values\n",
    "and numeric columns which contain different numeric values. It also has three more columns, the referral source that can have six different values,\n",
    "the diagnoses our target variable, and the record information representing only a unique identifier, so this column can be dropped.\n",
    "\n",
    "As to the missing values we will impute them later instead of deleting them, because they represent measures that where not taken, and\n",
    "not values that are truly missing."
   ],
   "id": "fa8740cab57e3e98"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Dropping the [record identification] column\n",
    "df_cleaned = df.drop('[record identification]', axis = 1)\n",
    "df_cleaned.info()"
   ],
   "id": "52483fdee69777d3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.3 Encoding Data",
   "id": "b225a2e435e1a905"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Now our data set is ready to be encoded. In this process, all the binary columns will be transformed\n",
    "into two 0's and 1's. The \"referral source\" column will be encoded using the method get_dummies from [Pandas](https://pandas.pydata.org).\n",
    "\n",
    "As for the target variable, it will be encoded according to 8 classes given to us in the file \"data.names\"."
   ],
   "id": "d2ad3ae71f2aae21"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "target = 'diagnoses'\n",
    "encoded_values = {\n",
    "    'M': '0', 'F': '1',\n",
    "    'f': '0', 't': '1'\n",
    "}\n",
    "\n",
    "df_target = pd.DataFrame(df_cleaned[target], columns=[target])\n",
    "encoded = df_cleaned.drop(target, axis=1).replace(encoded_values)\n",
    "df_encoded = pd.get_dummies(encoded, columns=['referral source:'], dtype='int')"
   ],
   "id": "5c5d778d1a1a0927",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Target variable enconding\n",
    "value_mapping = {\n",
    "    '-': 0,                          # healthy\n",
    "    'A': 1, 'B': 1, 'C': 1, 'D': 1,  # hyperthyroid conditions\n",
    "    'E': 2, 'F': 2, 'G': 2, 'H': 2,  # hypothyroid conditions\n",
    "    'I': 3, 'J': 3,                  # binding protein\n",
    "    'K': 4,                          # general health\n",
    "    'L': 5, 'M': 5, 'N': 5,          # replacement therapy\n",
    "    'R': 6,                          # discordant results\n",
    "}\n",
    "df_target[target] = df_target[target].map(value_mapping).fillna(7).astype(int)\n",
    "df_target[target].unique()"
   ],
   "id": "8bf89df76da8a4b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "df = pd.concat([df_encoded, df_target], axis=1)\n",
    "df.head()"
   ],
   "id": "ccac642a0fd480de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.4 Splitting into training and testing set",
   "id": "17c92ab69ddb4cab"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "With our data encoded, we are ready to split it into a training and testing sets.\n",
    "The training set will be used to train our classification model and the testing set will be used to test it."
   ],
   "id": "439616cb7b339cdc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.drop(target, axis='columns')\n",
    "y = df[target]\n",
    "\n",
    "X_TRAIN, X_TEST, y_TRAIN, y_TEST = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Print the shapes of the training and testing sets\n",
    "print('Training set shape:', X_TRAIN.shape, y_TRAIN.shape)\n",
    "print('Testing set shape:', X_TEST.shape, y_TEST.shape)"
   ],
   "id": "84054ef19a46bc36",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.5 Scaling Data",
   "id": "5a8b40e0871be938"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Because there are classification models that are based it the distance between the data, like KNN, \n",
    "it is important to normalize our training and testing sets."
   ],
   "id": "c607eae3ec7d4d0e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_TRAIN)\n",
    "\n",
    "X_train_scl = scaler.transform(X_TRAIN)\n",
    "X_test_scl = scaler.transform(X_TEST)"
   ],
   "id": "588e36dc53b771f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.6 Imputing missing values",
   "id": "c3b56e64c29f54aa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Again, because there are classification models that can not handle missing values, like KNN, \n",
    "we need to make the imputation of our NaN values."
   ],
   "id": "4ea066783dd98a80"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imputer.fit(X_train_scl)\n",
    "\n",
    "X_train_imp = imputer.transform(X_train_scl)\n",
    "X_test_imp = imputer.transform(X_test_scl)\n",
    "\n",
    "X_TRAIN = pd.DataFrame(X_train_imp, columns=X_TRAIN.columns)\n",
    "X_TEST = pd.DataFrame(X_test_imp, columns=X_TEST.columns)\n",
    "X_TRAIN.info()"
   ],
   "id": "e8524c87fdb9b95e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Classification Models",
   "id": "e70738c62e106345"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.1 Feature Selection",
   "id": "b61e2db8481d56a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO feature selection needs tuning\n",
    "from sklearn.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "selector = SFS(LinearRegression(), \n",
    "               n_features_to_select=13, \n",
    "               direction='forward',\n",
    "               n_jobs=-1)\n",
    "selector.fit(X_TRAIN, y_TRAIN)\n",
    "\n",
    "N, M = X_TRAIN.shape\n",
    "features=selector.get_support()\n",
    "features_selected = np.arange(M)[features]\n",
    "print(\"The features selected are columns: \", features_selected)\n",
    "\n",
    "X_TRAIN = selector.transform(X_TRAIN)\n",
    "X_TEST = selector.transform(X_TEST)"
   ],
   "id": "34d5b40cf1d9af3b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.2 KFold Cross validation",
   "id": "4d1cd5a9bbb0f86b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "def cross_validation(model, X, y):\n",
    "    TRUTH = None\n",
    "    PREDS = None\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, y_train = X[train_index], y.to_numpy()[train_index]\n",
    "        X_test, y_test = X[test_index], y.to_numpy()[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        if TRUTH is None:\n",
    "            PREDS = preds\n",
    "            TRUTH = y_test\n",
    "        else:\n",
    "            PREDS = np.hstack((PREDS, preds))\n",
    "            TRUTH = np.hstack((TRUTH, y_test))\n",
    "    return TRUTH, PREDS"
   ],
   "id": "dc8b24eb7dc939cb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def evaluate(model, X, y, n_iter=10):\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    mcc = []\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        truth, preds = cross_validation(model, X, y)\n",
    "        accuracy.append(accuracy_score(truth, preds))\n",
    "        precision.append(precision_score(truth, preds, average='weighted', zero_division=1))\n",
    "        recall.append(recall_score(truth, preds, average='weighted'))\n",
    "        f1.append(f1_score(truth, preds, average='weighted'))\n",
    "        mcc.append(matthews_corrcoef(truth, preds))\n",
    "        \n",
    "    return {\n",
    "        'Model': model,\n",
    "        'Accuracy': np.mean(accuracy),\n",
    "        'Precision': np.mean(precision),\n",
    "        'Recall': np.mean(recall),\n",
    "        'F1-Score': np.mean(f1),\n",
    "        'MCC': np.mean(mcc)\n",
    "    }"
   ],
   "id": "fe276d9166c129c2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 2.3 Classification models",
   "id": "5218522c6999b051"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "cols = ['Model', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'MCC']\n",
    "\n",
    "tree = evaluate(DecisionTreeClassifier(), X_TRAIN, y_TRAIN)\n",
    "lgr = evaluate(LogisticRegression(), X_TRAIN, y_TRAIN)\n",
    "naive_bayes = evaluate(GaussianNB(), X_TRAIN, y_TRAIN)\n",
    "knn = evaluate(KNeighborsClassifier(), X_TRAIN, y_TRAIN)\n",
    "svm = evaluate(SVC(), X_TRAIN, y_TRAIN)\n",
    "\n",
    "pd.DataFrame([tree, lgr, naive_bayes, knn, svm], columns=cols)"
   ],
   "id": "13d2e9541e1d907b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 3. Model Selection",
   "id": "f0e8c74fd37bd507"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.1 Hyperparameter tuning",
   "id": "2eb5fadc77b4e4d0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "model_params = {\n",
    "    'Decision Tree': {\n",
    "        'model': DecisionTreeClassifier(),\n",
    "        'params': {\n",
    "            'max_depth': [2, 3, 5, 10, 20],\n",
    "            'min_samples_split': [2, 3, 5, 10, 15],\n",
    "            'min_samples_leaf': [2, 3, 5, 10, 15],\n",
    "            'criterion': ['gini', 'entropy', 'log_loss']\n",
    "        }\n",
    "    },\n",
    "    'KNN': {\n",
    "        'model': KNeighborsClassifier(),\n",
    "        'params': {\n",
    "            'n_neighbors' : [3, 5, 7, 9, 11],\n",
    "            'weights' : ['uniform', 'distance'],\n",
    "            'metric' : ['minkowski', 'euclidean', 'manhattan']\n",
    "        }\n",
    "    },\n",
    "    'SVC': {\n",
    "        'model': SVC(),\n",
    "        'params': {\n",
    "            'C': [0.1, 1, 10, 100],  \n",
    "            'gamma': [1, 0.1, 0.01, 0.001], \n",
    "            'kernel': ['rbf', 'linear'] \n",
    "        }\n",
    "    }\n",
    "}"
   ],
   "id": "3ff3238a4ef5c21c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import time\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "scores = []\n",
    "start_time = time.time()\n",
    "for name, model in model_params.items():\n",
    "    grid_search = GridSearchCV(model['model'], model['params'], cv=5, n_jobs=-1)\n",
    "    grid_search.fit(X_TRAIN, y_TRAIN)\n",
    "    scores.append({\n",
    "        'Best Estimator': grid_search.best_estimator_,\n",
    "        'Best Score': grid_search.best_score_,\n",
    "        'Best Params': grid_search.best_params_\n",
    "    })\n",
    "    \n",
    "print('Computation time: %.2f' % (time.time() - start_time))\n",
    "df_tuning = pd.DataFrame(scores, columns=['Best Estimator', 'Best Score', 'Best Params'])\n",
    "df_tuning"
   ],
   "id": "1c996b4bb720017c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "tuned_reports = []\n",
    "for index, row in df_tuning.iterrows():\n",
    "    tuned_reports.append(evaluate(row['Best Estimator'], X_TRAIN, y_TRAIN))\n",
    "\n",
    "best_models = pd.DataFrame(tuned_reports, columns=cols)\n",
    "best_models"
   ],
   "id": "99a799adc0946b95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 3.2 Model Testing",
   "id": "5dbc196601e052de"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_report = []\n",
    "for index, row in best_models.iterrows():\n",
    "    model = row['Model']\n",
    "    model.fit(X_TRAIN, y_TRAIN)\n",
    "    preds = model.predict(X_TEST)\n",
    "    test_report.append({\n",
    "        'Model': model,\n",
    "        'Accuracy': accuracy_score(y_TEST, preds),\n",
    "        'Precision': precision_score(y_TEST, preds, average='weighted', zero_division=1),\n",
    "        'Recall': recall_score(y_TEST, preds, average='weighted'),\n",
    "        'F1-Score': f1_score(y_TEST, preds, average='weighted'),\n",
    "        'MCC': matthews_corrcoef(y_TEST, preds)\n",
    "    })\n",
    "\n",
    "models_test = pd.DataFrame(test_report, columns=cols)\n",
    "models_test"
   ],
   "id": "82f70294df161705",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
