{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e47ecd3f7a7681a",
   "metadata": {},
   "source": [
    "# Engenharia do Conhecimento 2023/2024\n",
    "\n",
    "## Project: *Thyroid disease Data Set*\n",
    "\n",
    "#### Group 6:\n",
    "\n",
    "- Eduardo Proen√ßa 57551\n",
    "- Tiago Oliveira 54979\n",
    "- Bernardo Lopes 54386\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b41ecdb1cdb3f9",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "To be done..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f652d741ef14fc0",
   "metadata": {},
   "source": [
    "## 1. Data processing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6609c1e4f2c7854",
   "metadata": {},
   "source": [
    "### 1.1 Creating a Data Frame\n",
    "\n",
    "Firstly, we need to create a Data Frame. Using the [Pandas](https://pandas.pydata.org) Python Library, we can read our data from the file proj-data.csv, which contains the data set we will be using in this project."
   ]
  },
  {
   "cell_type": "code",
   "id": "ff2b0f54da8e3a11",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load data set\n",
    "df_thyroid = pd.read_csv('proj-data.csv')\n",
    "df_thyroid.shape"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a6f5435c76fab441",
   "metadata": {},
   "source": [
    "df_thyroid.head()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.2 Data investigation",
   "id": "4a016ebe611ca03a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_thyroid.info()",
   "id": "3a5a2abac6b29bac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for col in df_thyroid.columns:\n",
    "    print(\"Values of \", end='')\n",
    "    print(df_thyroid[col].value_counts(), end=\"\\n\\n\")"
   ],
   "id": "972f648beb0eeca7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.3 Defining the train and target sets",
   "id": "12b9389aea7cd7fc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = df_thyroid.drop(\"[record identification]\", axis = 1)",
   "id": "7ad763ecd1af3116",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = df.drop(\"diagnoses\", axis='columns')\n",
    "y = df[\"diagnoses\"]"
   ],
   "id": "c9d97f580045cf89",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.4 Encoding our data",
   "id": "c7258171f85a871b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "object_cols = [\n",
    "    \"sex:\", \"on thyroxine:\", \"query on thyroxine:\", \n",
    "    \"on antithyroid medication:\", \"sick:\", \"pregnant:\",\n",
    "    \"thyroid surgery:\", \"I131 treatment:\", \"query hypothyroid:\",\n",
    "    \"query hyperthyroid:\", \"lithium:\", \"goitre:\", \"tumor:\", \n",
    "    \"hypopituitary:\", \"psych:\", \"TSH measured:\", \"T3 measured:\",\n",
    "    \"TT4 measured:\", \"T4U measured:\", \"FTI measured:\", \"TBG measured:\",\n",
    "    \"referral source:\"\n",
    "]\n",
    "\n",
    "numeric_cols = [\n",
    "    \"age:\", \"TSH:\", \"T3:\", \"TT4:\", \"T4U:\", \"FTI:\", \"TBG:\"\n",
    "]\n",
    "\n",
    "X.replace('?', np.nan, inplace=True)\n",
    "numeric = X.drop(object_cols, axis = 1)\n",
    "object = pd.get_dummies(X.drop(numeric_cols, axis = 1), dtype='int')\n",
    "X = pd.concat([object, numeric], axis = 1)\n",
    "X.head()"
   ],
   "id": "da8539aa45ffafb9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y)\n",
    "\n",
    "y = label_encoder.fit_transform(y)"
   ],
   "id": "a6aed30e6c0a98dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.5 Splitting",
   "id": "2fd74ac720283403"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_TRAIN, X_IVS, y_TRAIN, y_IVS = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "# Print the shapes of the training and testing sets\n",
    "print(\"Training set shape:\", X_TRAIN.shape, y_TRAIN.shape)\n",
    "print(\"Testing set shape:\", X_IVS.shape, y_IVS.shape)"
   ],
   "id": "5c7ff18bc63416ad",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "ac5504eec79e3204"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.5 Handling missing values",
   "id": "b0925eb072502419"
  },
  {
   "cell_type": "code",
   "id": "8998bc869e6da132",
   "metadata": {},
   "source": [
    "X_TRAIN = X_TRAIN.drop(\"TBG:\", axis='columns')\n",
    "X_IVS = X_IVS.drop(\"TBG:\", axis='columns')"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f05afa88d3fa8b95",
   "metadata": {},
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Initialize KNNImputer with k=5 (you can adjust k as needed)\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "\n",
    "# Perform KNN imputation\n",
    "X_train_imputed = imputer.fit_transform(X_TRAIN)\n",
    "X_ivs_imputed = imputer.transform(X_IVS)\n",
    "\n",
    "# Convert the imputed array back to a DataFrame\n",
    "X_TRAIN = pd.DataFrame(X_train_imputed, columns=X_TRAIN.columns)\n",
    "X_IVS = pd.DataFrame(X_ivs_imputed, columns=X_IVS.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 1.6 Scaling Data",
   "id": "f63f1bfec86d42e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_TRAIN)\n",
    "\n",
    "X_TRAIN_scl = scaler.transform(X_TRAIN)\n",
    "X_IVS_scl = scaler.transform(X_IVS)\n",
    "\n",
    "pd.DataFrame(X_TRAIN_scl, columns = X_TRAIN.columns).head()"
   ],
   "id": "ac697f942197163f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Classification Models",
   "id": "f95fe065b369236f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, matthews_corrcoef\n",
    "\n",
    "def evaluate(model):\n",
    "    TRUTH = None\n",
    "    PREDS = None\n",
    "    kf = KFold(n_splits=5, shuffle=True)\n",
    "    for train_index, test_index in kf.split(X_TRAIN):\n",
    "        X_train, X_test = X_TRAIN_scl[train_index], X_TRAIN_scl[test_index]\n",
    "        y_train, y_test = y_TRAIN[train_index], y_TRAIN[test_index]\n",
    "        \n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        if TRUTH is None:\n",
    "            PREDS = preds\n",
    "            TRUTH = y_test\n",
    "        else:\n",
    "            PREDS = np.hstack((PREDS, preds))\n",
    "            TRUTH = np.hstack((TRUTH, y_test))\n",
    "            \n",
    "    print(\"Cross validation statistics:\")\n",
    "    print(\"The Accuracy is: %7.4f\" % accuracy_score(TRUTH, PREDS))\n",
    "    print(\"The Precision is: %7.4f\" % precision_score(TRUTH, PREDS, average='weighted', zero_division=1))\n",
    "    print(\"The Recall is: %7.4f\" % recall_score(TRUTH, PREDS, average='weighted'))\n",
    "    print(\"The F1 score is: %7.4f\" % f1_score(TRUTH, PREDS, average='weighted'))\n",
    "    print(\"The Matthews correlation coefficient is: %7.4f\" % matthews_corrcoef(TRUTH, PREDS))"
   ],
   "id": "8082c1c51e946f3e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "evaluate(DecisionTreeClassifier(max_depth = 3))"
   ],
   "id": "ac0343e8e08c11a0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "evaluate(LogisticRegression(max_iter=1000))"
   ],
   "id": "18af00cbeaea0dce",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "evaluate(GaussianNB())"
   ],
   "id": "65d5c508cd6ed63c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "evaluate(KNeighborsClassifier(n_neighbors = 5, weights = \"distance\"))"
   ],
   "id": "8d030beec4acbfaf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "evaluate(SVC(kernel = \"rbf\", C = 1, gamma = 0.1))"
   ],
   "id": "81578d1646398cc7",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
